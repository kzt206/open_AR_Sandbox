{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox Sensor calibration\n",
    "In this tutorial we will learn the basic steps to set up the sensor for the _open AR Sandbox_. \n",
    "This not only involves tweaking of calibration parameters, but also the adjustment of your hardware component.\n",
    "\n",
    "Let's start with importing the main module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only useful when sandbox is not installed\n",
    "import os,sys\n",
    "sys.path.append('../../../')\n",
    "#\n",
    "from sandbox import _calibration_dir\n",
    "from sandbox.sensor import Sensor, CalibSensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before starting:\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b> Projector calibration file: </b> We will use the calibration file generated from the previous tutorial (Sandbox Projector calibration). If you have not calibrated yet the projector, please stop here and go back to the 1_calib_projector.ipynb jupyter notebook to calibrate the projector. If this is already done you can continue. \n",
    "</div>\n",
    "\n",
    "* **Sensor**: Provides us with a frame of distance data between the sensor and the sand surface.\n",
    "\n",
    "You can use different sensors, **KinectV1, KinectV2, LiDAR L515,Depthcamer D455, or a DummySensor** to simulate a topography and/or run other Modules. For the calibration process, this is realized in the CalibSensor class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supported sensor types:\n",
    "    # \"kinect_v1\"\n",
    "    # \"kinect_v2\"\n",
    "    # \"lidar\"\n",
    "    # \"D455\"\n",
    "    # \"dummy\"\n",
    "\n",
    "    \n",
    "    \n",
    "sensor_type = \"D455\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor calibration\n",
    "\n",
    "Your projector dashboard is calibrated and looks alright? Now, we need to calibrate the sensor so it exactly fits the projected main frame. First, we load the calibration file of the projector since this module needs to project the sensor image in the projector defined area. This will open a panel server with the sensor image updating, creating a simple topographic view with some color coded areas that guide you in finding the correct calibration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_proj = _calibration_dir + 'my_projector_calibration.json'\n",
    "module = CalibSensor(calibprojector = calib_proj, name = sensor_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sensor calibration is a bit more advanced than the projector positioning. In addition to the horizontal adjustment you also need to define vertical limits of the values, the sensor supplies. For example, this prevents unwanted model recalculations, when you move your hands above the projection area.\n",
    "\n",
    "**Adjust your hardware**\n",
    "\n",
    "If you call the calibrate_sensor() function you will see a current snapshot of a depth representation of your sandbox. It is easy to determine the sandboxe's edges and objects next to the sandbox, like the monitor or a chair. Position the Kinect sensor physically, so that the sensor is parallel to the sandbox surface and the outlines of the sandbox are parallel to the edges appears anymore. Each time, you have adjusted the hardware, take a new snapshot and check the visualization inside the calibration interface. Before a snapshot is taken, the software waits three seconds, allowing you to remove your hands or other objects between the sensor and the scanned surface.\n",
    "\n",
    "**Horizontal calibration**\n",
    "\n",
    "Afterwards, roughly adjust the first four margin sliders (blue) of the interface until the blue margin patches inside the snapshot cover the areas outisde the sandbox. Again, use the four corner poles as orientation. Those blue areas later will be cropped off by the software to focus on the area of interest.\n",
    "\n",
    "Now, you can adjust the four margins more precisely following the life representation inside the sandbox. Place a recognizable object like a cube or a cylinder inside the sandbox to check for possible offsets. Do that in the center of the surface, as well as close to the the edges of the sandbox.\n",
    "\n",
    "**Vertical calibration**\n",
    "\n",
    "The vertical value range that should be registered by the sensor, can be adjusted with the red and yellow sliders. The values represent the vertical distance away from the sensor in mm. Always make sure, the yellow slider is left of the red one to avoid confusion.\n",
    "\n",
    "Hold one hand right above the sandboxe's walls and move the yellow slider to the right until your hand is illuminated in yellow.\n",
    "\n",
    "To calibrate the vertical maximum of the range, dig a hole inside the sand until you reach the bottom of the sandbox. Now, move the red slider until only the bottom plate is colored red. Move the slider a little bit further to the right to increase the distance slightly below the sandbox. You can always follow this process inside the interface to also get a feedback on the distances of surrounding objects behind the blue margin patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "widget = module.calibrate_sensor()\n",
    "widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have successfully calibrated your sandbox, and therefore, are ready to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the projector calibration, we will save the sensor calibration in a JSON file for future reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module.sensor.save_json(file=_calibration_dir+ 'my_sensor_calibration.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the sensor is correctly calibrated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next time you start the software, simply pass the file's location and name as an argument to the Sensor instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor = Sensor(calibsensor=_calibration_dir+ 'my_sensor_calibration.json', name = sensor_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(sensor.get_frame(), origin=\"lower\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "print(sensor.extent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
